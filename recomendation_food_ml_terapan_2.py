# -*- coding: utf-8 -*-
"""Recomendation_food_ML_Terapan_2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/silvii30/recomendation-food-ml-terapan-2.f68954e2-0de6-4f93-b2dc-78c3a3fbdfa2.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250523/auto/storage/goog4_request%26X-Goog-Date%3D20250523T165911Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db33e6e0778ef43185834ca0b2db13c3f9318c5b3314691c0f21239837d88fb166d4f5c1045cbf96f161935f7f3d2a51163f884e028c8da71f34b4d6aa98a43b50f0847f040d568a59b2b8e93d061af26480724947e24016893c263ead0824723fb46df951439614f035013299c5e357e3808ee8194f8242318f9bd3c30f65be8debf930b3ce8e292759a001f50193b50cbd58e87b8a181b31307157f931f430842290d103a517e2d7595fe0273b95e2688903d1480440e3754a1ab92b26c4f828d2e5d3b6ace1b5f1750c257bce854d03cf08ac51047dc0a4148fc6b1f4ae3e00835a9f47c738580df23e48555ede22c52c7902a5e93bdb31f7062f446ec30bb

# Rekomendasi Makanan Berbasis Content-Based dan Collaborative Filtering

# Dataset dan Referensi

**Dataset:** [Food.com Recipes and Interactions Dataset](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions)

* Berisi lebih dari 700.000 interaksi pengguna dan 180.000+ resep
* Dua file utama digunakan:

  * `RAW_recipes.csv` — metadata resep
  * `RAW_interactions.csv` — review, rating, dan user ID

**Referensi Akademik:**

* Yi-Ying Chow et al., *Food Recommender System: A Review*, Journal of System and Management Sciences, 2023
* Jon Bondevik et al., *A Systematic Review on Food Recommender Systems*, Elsevier, 2023
* Muhamad Naufal Syaiful Bahri et al., *Implementasi Sistem Rekomendasi Makanan EatAja*, Multinetics, 2021

# Import Library
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
import joblib
import pickle
import os

# NLP & similarity metrics
from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS
from sklearn.neighbors import NearestNeighbors

# Untuk Collaborative Filtering
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Model
from tensorflow.keras import layers, regularizers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import mean_squared_error

import warnings
warnings.filterwarnings('ignore')

"""**Tahap 1: Import Library**

* `pandas`, `numpy`: manipulasi data
* `matplotlib`, `seaborn`: visualisasi
* `sklearn`: vectorisasi teks, evaluasi metrik, encoding ID
* `tensorflow`: deep learning untuk collaborative filtering

# Load Dataset
"""

for dirname, _, filenames in os.walk('/kaggle/input'):

    for filename in filenames:
        print(os.path.join(dirname, filename))

interactions = pd.read_csv('/kaggle/input/food-com-recipes-and-user-interactions/RAW_interactions.csv')
recipes = pd.read_csv('/kaggle/input/food-com-recipes-and-user-interactions/RAW_recipes.csv')

"""**Tahap 2: Load Dataset**

* Data resep digunakan untuk content-based filtering
* Data interaksi digunakan untuk collaborative filtering

# EDA
"""

print(interactions.head())

"""- menampilkan 5 baris data pertama di data interactions"""

print(recipes.head())

"""- menampilkan 5 baris data pertama di data recipes"""

interactions.isna().sum()

"""- mencari tau missing value di data interactions
- terdapat missing value di kolom review
"""

recipes.isna().sum()

"""- mencari tau missing value di data recipes
- terdapat missing value di kolom name dan description
"""

interactions.duplicated().sum()

"""- tidak ada data duplikat di data interactions"""

recipes.duplicated().sum()

"""- tidak ada data duplikat di data recipes"""

interactions.info()

"""**Penjelasan:**
Digunakan untuk memahami struktur data: jumlah kolom, jenis tipe data, dan jumlah data non-null.

**interactions:**

* 5 kolom: user\_id, recipe\_id, date, rating, review
* 1.132.367 entri total
* Kolom bertipe kombinasi `int64` dan `object`
"""

recipes.info()

"""**recipes:**

* 12 kolom termasuk name, nutrition, ingredients, etc
* 231.637 entri total
* Kolom bertipe campuran `int64` dan `object`
"""

sns.histplot(interactions['rating'], bins=20)
plt.title('Distribusi Rating')
plt.xlabel('Rating')
plt.ylabel('Frekuensi')
plt.show()

"""**Insight:**
Distribusi rating sangat tidak seimbang dengan dominasi rating 5. Hal ini menunjukkan bahwa pengguna cenderung hanya memberi nilai tinggi.
"""

user_counts = interactions['user_id'].value_counts()
sns.histplot(user_counts, bins=50)
plt.title('Jumlah Rating per User')
plt.xlabel('Jumlah Rating')
plt.ylabel('Jumlah User')
plt.show()

"""**Insight:**
Sebagian besar pengguna memberikan sedikit rating, menunjukkan ketimpangan aktivitas antar pengguna.
"""

sns.boxplot(x=interactions['rating'])
plt.title('Boxplot Rating (Outlier Detection)')
plt.show()

"""**Insight:**
Boxplot menunjukkan adanya beberapa outlier terutama pada rating rendah (0-2), walau mayoritas rating berada pada kisaran tinggi.
"""

nutrition_df = recipes['nutrition'].str.strip('[]').str.split(',', expand=True).astype(float)
nutrition_df.columns = ['calories', 'total_fat', 'sugar', 'sodium', 'protein', 'saturated_fat', 'carbs']
sns.heatmap(nutrition_df.corr(), annot=True, cmap='coolwarm')
plt.title('Korelasi antar Fitur Nutrisi')
plt.show()

"""**Insight:**
Terdapat korelasi tinggi antara sugar dan carbs, serta total_fat dengan saturated_fat
"""

sns.histplot(recipes['n_ingredients'], bins=30)
plt.title('Distribusi Jumlah Bahan Resep')
plt.xlabel('Jumlah Bahan')
plt.ylabel('Jumlah Resep')
plt.show()

"""**Insight:**
Mayoritas resep memiliki 7–13 bahan. Ini dapat menjadi indikator kompleksitas resep yang disukai pengguna.

# Data Preprosessing
"""

recipes['description'] = recipes['description'].fillna('No description provided by user.')
recipes['name'] = recipes['name'].fillna('Unnamed Recipe')
recipes['ingredients'] = recipes['ingredients'].fillna("['No ingredients listed']")
interactions['review'] = interactions['review'].fillna('No review provided by user.')

"""**Data Preprocessing - Handling Missing Values**

Pada tahap ini, dilakukan pembersihan data dengan mengisi nilai kosong (missing values) pada kolom-kolom penting di dataset agar siap untuk tahap ekstraksi fitur dan pelatihan model.

**Penyesuaian yang dilakukan:**


- **`description`**: Diisi dengan `"No description provided by user."` untuk menjaga konsistensi pada input deskripsi resep.
- **`name`**: Diisi dengan `"Unnamed Recipe"` sebagai penanda jika nama resep tidak tersedia.
- **`ingredients`**: Diisi dengan list string `"['No ingredients listed']"` untuk menjaga struktur saat dilakukan parsing dan tokenisasi bahan.
- **`review`**: Diisi dengan `"No review provided by user."` untuk mempermudah analisis teks atau agregasi feedback.

# Feature Engineering - Content-Based Representation
"""

recipes['content'] = recipes['name'] + ' ' + recipes['description'] + ' ' + recipes['ingredients'].apply(lambda x: ' '.join(eval(x)))

stop_words = ENGLISH_STOP_WORDS

recipes = recipes.dropna(subset=['content'])

vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
tfidf_matrix = vectorizer.fit_transform(recipes['content'])

"""1. **Pembuatan Fitur `content`**:
   - Tiga kolom utama yaitu `name`, `description`, dan `ingredients` digabung menjadi satu string teks panjang yang disebut sebagai `content`.
   - Kolom `ingredients` diparsing dari string list menjadi bentuk list asli menggunakan `eval`, lalu digabung menggunakan spasi.
   - Ini menghasilkan deskripsi utuh setiap resep yang mencakup judul, penjelasan, dan daftar bahan.

2. **Penghapusan Nilai Kosong**:
   - Meskipun sebelumnya missing values telah diisi, sebagai jaga-jaga dilakukan penghapusan baris yang memiliki nilai kosong pada kolom `content` untuk memastikan kualitas input vektorisasi.

3. **TF-IDF Vectorization**:
   - Fitur `content` kemudian dikonversi ke bentuk numerik menggunakan **TF-IDF (Term Frequency-Inverse Document Frequency)**.
   - `TfidfVectorizer` digunakan dengan parameter:
     - `max_features=5000`: membatasi jumlah fitur agar tetap efisien.
     - `stop_words='english'`: menghilangkan kata-kata umum (stopwords) dalam bahasa Inggris agar tidak mendominasi representasi.
   - Hasil akhir adalah matriks sparse berdimensi `[jumlah_resep x 5000]` yang menggambarkan bobot pentingnya kata-kata dalam setiap resep.

# Content Based Filtering (Modelling dan Rekomendasi)
"""

name_to_index = pd.Series(recipes.index, index=recipes['name']).drop_duplicates()
nn_model = NearestNeighbors(metric='cosine', algorithm='brute')
nn_model.fit(tfidf_matrix)

def recommend_content(name, top_n=5):
    idx = name_to_index[name]
    distances, indices = nn_model.kneighbors(tfidf_matrix[idx], n_neighbors=top_n + 1)
    indices = indices.flatten()[1:]
    return recipes.iloc[indices][['id', 'name', 'description']]

"""Setelah TF-IDF matrix berhasil dibentuk, langkah selanjutnya adalah membangun sistem rekomendasi berbasis kemiripan antar resep.

1. **Mapping Nama Resep ke Indeks**
   - `name_to_index` digunakan untuk membuat pemetaan dari nama resep ke indeks baris pada DataFrame `recipes`.
   - Hal ini memungkinkan pengguna memasukkan nama resep sebagai input saat meminta rekomendasi.

2. **Model Nearest Neighbors**
   - `NearestNeighbors` dari `sklearn` digunakan dengan metrik **cosine similarity**, metode umum dalam content-based filtering.
   - Parameter `algorithm='brute'` digunakan karena ukuran data besar dan cosine similarity tidak bisa dioptimalkan dengan k-d tree atau ball tree.

3. **Fungsi Rekomendasi: `recommend_content(name)`**
   - Fungsi menerima nama resep sebagai input.
   - Mencari resep dengan TF-IDF yang paling mirip (berdasarkan cosine similarity).
   - Mengembalikan `top_n` resep serupa (selain dirinya sendiri), termasuk kolom `id`, `name`, dan `description` untuk konteks pengguna.

# Inference
"""

print("Rekomendasi makanan yang mirip 'Cream of Spinach Soup':")
print(recommend_content("cream of spinach soup"))

"""Inference menggunakan data input cream of spinach soup"""

print("Rekomendasi makanan yang mirip 'fried potatoes':")
print(recommend_content("fried potatoes"))

"""Inference menggunakan data input fried potatoes"""

print("Rekomendasi makanan yang mirip 'pasta':")
print(recommend_content("pasta"))

"""Inference menggunakan data input pasta"""

# Simpan vectorizer dan matrix
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')
joblib.dump(tfidf_matrix, 'tfidf_matrix.pkl')

# Simpan model NearestNeighbors
joblib.dump(nn_model, 'nearest_neighbors_model.pkl')

# Simpan mapping indeks
with open('name_to_index.pkl', 'wb') as f:
    pickle.dump(name_to_index, f)

"""**Save Model**

1. **TF-IDF Vectorizer**
   - Disimpan menggunakan `joblib` sebagai `tfidf_vectorizer.pkl`.
   - Digunakan untuk mengubah teks resep baru ke dalam vektor fitur.

2. **TF-IDF Matrix**
   - Matrix hasil transformasi semua resep disimpan sebagai `tfidf_matrix.pkl`.
   - Berguna untuk inference ulang atau jika ingin menambahkan rekomendasi baru tanpa mengulang proses training.

3. **Model Nearest Neighbors**
   - Objek model `NearestNeighbors` disimpan sebagai `nearest_neighbors_model.pkl`.
   - Ini memungkinkan kita untuk memuat model dan langsung melakukan rekomendasi di sesi atau aplikasi lain.

4. **Mapping Nama ke Indeks**
   - Dictionary `name_to_index` digunakan untuk mencari indeks resep berdasarkan nama input.
   - Disimpan sebagai file pickle `name_to_index.pkl`.

# Data Preprocessing - Collaborative Filtering
"""

user_encoder = LabelEncoder()
item_encoder = LabelEncoder()

interactions['user'] = user_encoder.fit_transform(interactions['user_id'])
interactions['item'] = item_encoder.fit_transform(interactions['recipe_id'])

user_encoded_to_user = dict(enumerate(user_encoder.classes_))
item_encoded_to_item = dict(enumerate(item_encoder.classes_))

min_rating = interactions['rating'].min()
max_rating = interactions['rating'].max()

interactions['rating_normalized'] = interactions['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating))

X = interactions[['user', 'item']].values
y = interactions['rating_normalized'].values

x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

y_train = y_train.reshape(-1, 1)
y_val = y_val.reshape(-1, 1)

"""1. **Encoding User dan Item ID**
   - `user_id` dan `recipe_id` diubah menjadi indeks numerik menggunakan `LabelEncoder`.
   - Model deep learning membutuhkan input integer untuk digunakan dalam layer embedding.
   - Disimpan juga mapping `user_encoded_to_user` dan `item_encoded_to_item` untuk keperluan decoding hasil model.

2. **Normalisasi Rating**
   - Skor rating dinormalisasi ke skala [0, 1] menggunakan metode Min-Max Scaling.
   - Tujuannya agar target rating kompatibel dengan output model (aktivasi sigmoid di akhir).

3. **Membentuk Dataset X dan y**
   - `X`: kombinasi pasangan `[user, item]` untuk setiap baris interaksi.
   - `y`: target prediksi berupa nilai rating yang telah dinormalisasi.

4. **Split Dataset**
   - Data dibagi menjadi data latih (`x_train`, `y_train`) dan validasi (`x_val`, `y_val`) menggunakan rasio 80:20.
   - Ini memungkinkan model dievaluasi terhadap data yang tidak dilihat saat pelatihan.

5. **Reshape Target**
   - Rating disusun ulang ke format `n x 1` agar sesuai dengan bentuk output model.

# Collaborative Filtering (modelling dan rekomendasi)
"""

class RecommenderNet(Model):
    def __init__(self, num_users, num_items, embedding_size=50, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(
            num_users, embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.item_embedding = layers.Embedding(
            num_items, embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-6)
        )
        self.item_bias = layers.Embedding(num_items, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        item_vector = self.item_embedding(inputs[:, 1])
        item_bias = self.item_bias(inputs[:, 1])
        dot_user_item = tf.tensordot(user_vector, item_vector, 2)
        x = dot_user_item + user_bias + item_bias
        return x

"""**Arsitektur Model:**

1. **Embedding Layer untuk User dan Item**
   - `user_embedding`: mengubah user ID menjadi vektor berdimensi `embedding_size`
   - `item_embedding`: mengubah item ID (resep) menjadi vektor embedding
   - Digunakan `he_normal` sebagai inisialisasi bobot dan regularisasi L2 untuk mencegah overfitting

2. **Bias Layer**
   - `user_bias` dan `item_bias`: menangkap kecenderungan rata-rata dari masing-masing user dan item terhadap rating
   - Ditambahkan langsung ke prediksi akhir untuk memperbaiki hasil model

3. **Dot Product**
   - Prediksi utama dilakukan dengan menghitung **dot product** antara vektor user dan item
   - Hasil dot product mencerminkan seberapa cocok (similar) preferensi pengguna terhadap item

4. **Output**
   - Hasil akhir adalah penjumlahan antara dot product dan bias
   - Tidak menggunakan aktivasi (seperti sigmoid) karena regresi rating dilakukan langsung pada nilai numerik
"""

num_users = len(user_encoder.classes_)
num_items = len(item_encoder.classes_)

model = RecommenderNet(num_users, num_items, embedding_size=50)
model.compile(
    loss='mse',
    optimizer=Adam(learning_rate=0.0001),
    metrics=[RootMeanSquaredError()]
)
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    x_train, y_train,
    batch_size=64,
    epochs=20,
    validation_data=(x_val, y_val),
    callbacks=[early_stop]
)

"""1. **Jumlah Pengguna dan Item**
   - `num_users` dan `num_items` diperoleh dari hasil encoding.
   - Digunakan sebagai ukuran input untuk layer embedding.

2. **Kompilasi Model**
   - **Loss Function**: `mean squared error (MSE)` digunakan karena target berupa rating numerik kontinu.
   - **Optimizer**: `Adam` dengan `learning_rate=0.0001`, cocok untuk training model embedding.
   - **Metric**: `RootMeanSquaredError (RMSE)` dipilih karena lebih intuitif untuk evaluasi skala rating.

3. **Early Stopping**
   - Menggunakan `EarlyStopping` untuk menghentikan pelatihan saat model tidak mengalami perbaikan pada `val_loss` selama 3 epoch berturut-turut.
   - `restore_best_weights=True` memastikan model terbaik disimpan berdasarkan evaluasi validasi.

4. **Training**
   - Model dilatih selama maksimum 20 epoch dengan `batch_size=64`.
   - Validasi dilakukan menggunakan `x_val` dan `y_val` untuk memantau overfitting.

# Evaluasi Model
"""

y_pred = model.predict(x_val)
mse = mean_squared_error(y_val, y_pred)
rmse = np.sqrt(mse)

print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")

"""**Metrik Evaluasi:**

1. **Mean Squared Error (MSE)**  
   MSE menghitung rata-rata dari kuadrat selisih antara nilai prediksi dan nilai aktual. Semakin kecil nilai MSE, semakin baik prediksi model.

2. **Root Mean Squared Error (RMSE)**  
   RMSE adalah akar dari MSE dan mengembalikan hasil dalam satuan yang sama dengan target (dalam hal ini: skor rating). RMSE lebih interpretatif dalam konteks kesalahan karena menunjukkan rata-rata jarak prediksi dari nilai sebenarnya.

**Hasil Evaluasi:**

- **MSE:** 0.2130  
- **RMSE:** 0.4615
"""

plt.figure(figsize=(12, 5))

# Loss Plot
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training & Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE')
plt.legend()

# RMSE Plot
plt.subplot(1, 2, 2)
plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.title('Training & Validation RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()

plt.tight_layout()
plt.show()

"""**Insight dari Grafik:**

- **Grafik Kiri (Training & Validation Loss):**
  - MSE pada data training terus menurun seiring bertambahnya epoch, menunjukkan model berhasil belajar dari data pelatihan.
  - MSE pada data validasi relatif stabil setelah epoch ke-2, yang mengindikasikan tidak terjadi overfitting selama proses pelatihan.

- **Grafik Kanan (Training & Validation RMSE):**
  - RMSE pada training menurun secara konsisten, memperlihatkan peningkatan akurasi model dalam memprediksi rating pengguna.
  - RMSE validasi juga menunjukkan tren penurunan, walau sedikit stagnan di beberapa epoch terakhir. Hal ini menandakan bahwa model cukup generalizable dan tidak hanya menghafal data training.

**Kesimpulan:**

Visualisasi ini menunjukkan bahwa model telah dilatih secara efektif tanpa overfitting yang signifikan. Selisih antara RMSE training dan validasi tidak terlalu besar, yang menandakan model memiliki performa yang stabil dan mampu melakukan generalisasi dengan baik terhadap data baru.

# Inference
"""

def recommend_for_user(user_id_raw, top_n=5):
    try:
        user_idx = user_encoder.transform([user_id_raw])[0]
    except ValueError:
        print(f"User ID {user_id_raw} tidak ditemukan dalam data training.")
        return pd.DataFrame()

    item_indices = np.arange(num_items)
    user_input = np.full_like(item_indices, user_idx)

    predictions = model.predict(np.stack([user_input, item_indices], axis=1), verbose=0)

    top_indices = predictions.flatten().argsort()[::-1][:top_n]
    recommended_item_ids = item_encoder.inverse_transform(top_indices)

    return recipes[recipes['id'].isin(recommended_item_ids)][['id', 'name', 'description']].reset_index(drop=True)

"""1. **Transformasi ID Pengguna:**

   * `user_id_raw` diubah menjadi bentuk numerik menggunakan `LabelEncoder` agar bisa dikenali oleh model.
   * Jika `user_id_raw` tidak tersedia dalam data pelatihan, fungsi mengembalikan DataFrame kosong.

2. **Pembuatan Input Prediksi:**

   * Dibuat pasangan kombinasi user-item dengan semua item yang tersedia untuk memprediksi ratingnya sekaligus.

3. **Prediksi Rating oleh Model:**

   * Model melakukan prediksi terhadap seluruh kombinasi item-user dalam bentuk skor prediksi (rating terstandardisasi).

4. **Seleksi Top-N Item:**

   * Mengurutkan hasil prediksi dan memilih `top_n` makanan dengan skor tertinggi.

5. **Keluaran Rekomendasi:**

   * Mengembalikan daftar makanan dengan `id`, `name`, dan `description` berdasarkan ID item yang telah dipetakan kembali ke aslinya.
"""

user_id_input = 8937
print(f"Rekomendasi untuk User ID {user_id_input} berdasarkan Collaborative Filtering:")
hasil = recommend_for_user(user_id_input, top_n=5)
print("Pesanan Rekomendasi:")
print(hasil[['name', 'description']])

"""**Interpretasi dan Insight:**

* Rekomendasi ini didasarkan pada pola rating historis pengguna terhadap makanan serupa.
* Beberapa makanan adalah **kontes entry**, menunjukkan kemungkinan bahwa user menyukai eksperimen rasa baru dan konten kompetitif.
* Ciri umum lainnya adalah kombinasi **makanan gurih dan inovatif** – muffin asin, salad tropis, dan makanan Meksiko berlapis.
"""

model.save("recommendasi_model.keras")

"""Model disimpan dalam format .keras"""